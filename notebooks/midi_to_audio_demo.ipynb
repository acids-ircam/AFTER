{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI-to-audio generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "\n",
    "gin.enter_interactive_mode()\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "step = 0\n",
    "autoencoder_path = \"\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate te model and load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from after.diffusion import RectifiedFlow\n",
    "\n",
    "checkpoint_path = model_path + \"/checkpoint\" + str(step) + \"_EMA.pt\"\n",
    "config = os.path.join(model_path, \"config.gin\")\n",
    "\n",
    "# Parse config\n",
    "gin.parse_config_file(config)\n",
    "SR = gin.query_parameter(\"%SR\")\n",
    "n_signal = gin.query_parameter(\"%N_SIGNAL\")\n",
    "\n",
    "# Emb model\n",
    "\n",
    "# Instantiate model\n",
    "blender = RectifiedFlow(device=device)\n",
    "\n",
    "# Load checkpoints\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"model_state\"]\n",
    "blender.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Emb model\n",
    "emb_model = torch.jit.load(autoencoder_path).eval()\n",
    "blender.emb_model = emb_model\n",
    "\n",
    "# Send to device\n",
    "blender = blender.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from after.dataset import SimpleDataset\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "db_path = \"\"\n",
    "dataset = SimpleDataset(path=db_path, keys=[\"z\", \"midi\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dataset[0]\n",
    "d2 = dataset[1]\n",
    "\n",
    "z1 = d1[\"z\"][..., :n_signal]  # guitar\n",
    "z2 = d2[\"z\"][..., :n_signal]\n",
    "\n",
    "z1, z2 = torch.tensor(z1).to(device).unsqueeze(0), torch.tensor(z2).to(\n",
    "    device).unsqueeze(0)\n",
    "\n",
    "\n",
    "def normalize(array):\n",
    "    return (array - array.min()) / (array.max() - array.min() + 1e-6)\n",
    "\n",
    "\n",
    "ae_ratio = gin.query_parameter(\"utils.collate_fn.ae_ratio\")\n",
    "full_length = dataset[0][\"z\"].shape[-1]\n",
    "times = times = np.linspace(0, full_length * ae_ratio / SR, full_length)\n",
    "\n",
    "midis = [d1[\"midi\"], d2[\"midi\"]]\n",
    "pr = [m.get_piano_roll(times=times) for m in midis]\n",
    "pr = map(normalize, pr)\n",
    "pr = np.stack(list(pr))\n",
    "pr = pr[..., :n_signal]\n",
    "\n",
    "pr = torch.from_numpy(pr).float().unsqueeze(1).to(device)\n",
    "\n",
    "pr1, pr2 = pr\n",
    "\n",
    "x1, x2 = blender.emb_model.decode(\n",
    "    z1).cpu().squeeze(), blender.emb_model.decode(z2).cpu().squeeze()\n",
    "\n",
    "display(Audio(x1, rate=SR))\n",
    "plt.imshow(pr1[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "display(Audio(x2, rate=SR))\n",
    "plt.imshow(pr2[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = 20  #Number of diffusion steps\n",
    "guidance = 1.0  #Classifier free guidance strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute timbre representation\n",
    "zsem1, zsem2 = blender.encoder(z1), blender.encoder(z2)\n",
    "\n",
    "time_cond = pr1\n",
    "zsem = zsem2\n",
    "\n",
    "# Sample initial noise\n",
    "x0 = torch.randn_like(z1)\n",
    "\n",
    "print(\"Normal\")\n",
    "total_guidance = 1.0\n",
    "guidance_joint_factor = 1.0\n",
    "guidance_cond_factor = 0\n",
    "\n",
    "xS = blender.sample(\n",
    "    x0,\n",
    "    time_cond=time_cond,\n",
    "    cond=zsem,\n",
    "    nb_steps=nb_steps,\n",
    ")\n",
    "\n",
    "audio_out = blender.emb_model.decode(xS).cpu().numpy().squeeze()\n",
    "display(Audio(audio_out, rate=SR))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_after",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
