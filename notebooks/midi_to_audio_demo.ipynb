{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI-to-audio generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "\n",
    "gin.enter_interactive_mode()\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Model can be loaded from a training checkpoint (.pt file stored by default in ./after_runs/#model_name), or from an already exported .ts file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model from a training Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "step = None  # Use None to load the last\n",
    "autoencoder_path = \"\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate te model and load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, emb_model, device=device):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.emb_model = emb_model\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def ae_encode(self, x):\n",
    "        if len(x.shape) > 1:\n",
    "            x = x.reshape(x.shape[0], 1, -1)  # Flatten the input\n",
    "        else:\n",
    "            x = x.reshape(1, 1, -1)\n",
    "\n",
    "        return self.emb_model.encode(x.to(self.device))\n",
    "\n",
    "    def ae_decode(self, z):\n",
    "        return self.emb_model.decode(z.to(self.device)).cpu().squeeze()\n",
    "\n",
    "    def timbre(self, z):\n",
    "        return self.model.encoder(z.to(self.device))\n",
    "\n",
    "    def sample(self, noise, z_structure, z_timbre, guidance_timbre,\n",
    "               guidance_structure, nb_steps):\n",
    "\n",
    "        zout = self.model.sample(noise.to(self.device),\n",
    "                                 time_cond=z_structure.to(self.device),\n",
    "                                 cond=z_timbre.to(self.device),\n",
    "                                 nb_steps=nb_steps,\n",
    "                                 guidance_structure=guidance_structure,\n",
    "                                 guidance_timbre=guidance_timbre)\n",
    "        return zout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from after.diffusion import RectifiedFlow\n",
    "\n",
    "if step is None:\n",
    "    files = os.listdir(model_path)\n",
    "    files = [f for f in files if f.startswith(\"checkpoint\")]\n",
    "    steps = [f.split(\"_\")[-2].replace(\"checkpoint\", \"\") for f in files]\n",
    "    step = max([int(s) for s in steps])\n",
    "    checkpoint_file = \"checkpoint\" + str(step) + \"_EMA.pt\"\n",
    "else:\n",
    "    checkpoint_file = \"checkpoint\" + str(step) + \"_EMA.pt\"\n",
    "\n",
    "checkpoint_file = os.path.join(model_path, checkpoint_file)\n",
    "config = os.path.join(model_path, \"config.gin\")\n",
    "\n",
    "# Parse config\n",
    "gin.parse_config_file(config)\n",
    "SR = gin.query_parameter(\"%SR\")\n",
    "n_signal = gin.query_parameter(\"%N_SIGNAL\")\n",
    "latent_size = gin.query_parameter(\"%IN_SIZE\")\n",
    "# Emb model\n",
    "\n",
    "# Instantiate model\n",
    "blender = RectifiedFlow(device=device)\n",
    "\n",
    "# Load checkpoints\n",
    "state_dict = torch.load(checkpoint_file, map_location=\"cpu\")[\"model_state\"]\n",
    "blender.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Emb model\n",
    "emb_model = torch.jit.load(autoencoder_path).eval()\n",
    "\n",
    "dummy = torch.randn(1, 1, 8192)  # Dummy input for model initialization\n",
    "z = emb_model.encode(dummy)\n",
    "\n",
    "ae_ratio = dummy.shape[-1] / z.shape[-1]\n",
    "\n",
    "# Send to device\n",
    "blender = blender.eval()\n",
    "\n",
    "model = CheckpointModel(blender, emb_model, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model from a torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_path = \"\"\n",
    "autoencoder_path = \"\"\n",
    "\n",
    "ts_model = torch.jit.load(ts_path)\n",
    "emb_model = torch.jit.load(autoencoder_path).eval()\n",
    "# You can set None if the autoencoder path is not known, but it will use the streaming model embedded in the .ts which has some latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ts_model, emb_model=None):\n",
    "        super().__init__()\n",
    "        self.model = ts_model\n",
    "        self.emb_model = emb_model\n",
    "\n",
    "    def ae_encode(self, x):\n",
    "        if len(x.shape) > 1:\n",
    "            x = x.reshape(x.shape[0], 1, -1)  # Flatten the input\n",
    "        else:\n",
    "            x = x.reshape(1, 1, -1)\n",
    "\n",
    "        if self.emb_model is not None:\n",
    "            return self.emb_model.encode(x)\n",
    "        return self.model.emb_model_structure.encode(x)\n",
    "\n",
    "    def ae_decode(self, z):\n",
    "        if self.emb_model is not None:\n",
    "            return self.emb_model.decode(z).squeeze().cpu()\n",
    "        return self.model.emb_model_timbre.decode(z).squeeze().cpu()\n",
    "\n",
    "    def timbre(self, z):\n",
    "        return self.model.encoder.forward_stream(z)\n",
    "\n",
    "    def structure(self, z):\n",
    "        return self.model.encoder_time.forward_stream(z)\n",
    "\n",
    "    def sample(self, noise, z_structure, z_timbre, guidance_timbre,\n",
    "               guidance_structure, nb_steps):\n",
    "\n",
    "        self.model.set_guidance_timbre(guidance_timbre)\n",
    "        self.model.set_guidance_structure(guidance_structure)\n",
    "        self.model.set_nb_steps(nb_steps)\n",
    "\n",
    "        zout = self.model.sample(noise, time_cond=z_structure, cond=z_timbre)\n",
    "\n",
    "        return zout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSModel(ts_model, emb_model=emb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Samples\n",
    "\n",
    "You can either load a sample from a prepared lmdb database, or directly from audio files (below). By defaut, audio files are cut to the training length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load audio from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from after.dataset import SimpleDataset\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "db_path = \"\"\n",
    "dataset = SimpleDataset(path=db_path, keys=[\"z\", \"midi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dataset[380]  # Example for timbre\n",
    "d2 = dataset[540]  # Example for midi\n",
    "\n",
    "z1 = d1[\"z\"][..., :n_signal]  # guitar\n",
    "z2 = d2[\"z\"][..., :n_signal]\n",
    "\n",
    "z1, z2 = torch.tensor(z1).unsqueeze(0), torch.tensor(z2).unsqueeze(0)\n",
    "\n",
    "\n",
    "def normalize(array):\n",
    "    return (array - array.min()) / (array.max() - array.min() + 1e-6)\n",
    "\n",
    "\n",
    "ae_ratio = gin.query_parameter(\"utils.collate_fn.ae_ratio\")\n",
    "full_length = dataset[0][\"z\"].shape[-1]\n",
    "times = times = np.linspace(0, full_length * ae_ratio / SR, full_length)\n",
    "pr = d1[\"midi\"].get_piano_roll(times=times)\n",
    "pr = pr / 127\n",
    "pr = pr[..., :n_signal]\n",
    "pr = torch.from_numpy(pr).float().reshape(1, 128, -1)\n",
    "\n",
    "x1, x2 = model.ae_decode(z1), model.ae_decode(z2)\n",
    "\n",
    "print(\"Audio for timbre\")\n",
    "display(Audio(x1, rate=SR))\n",
    "plt.show()\n",
    "\n",
    "print(\"Midi Example\")\n",
    "display(Audio(x2, rate=SR))\n",
    "plt.imshow(pr[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load audio from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"\"\n",
    "midi_path = \"\"\n",
    "\n",
    "offset_midi = 0  #Start moment in the midi file\n",
    "duration = 10  # Duration of the midi chunk and generated audio = the model will generate audio based on the midi information between offset and offset + duration\n",
    "\n",
    "offset_audio = 0  #Start moment in the audio file. Duration will be based on the signal length seen by the model during training\n",
    "duration_audio = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, _ = librosa.load(audio_path,\n",
    "                     sr=SR,\n",
    "                     mono=True,\n",
    "                     offset=offset_audio,\n",
    "                     duration=10)\n",
    "\n",
    "print(x1.shape)\n",
    "display(Audio(x1, rate=SR))\n",
    "\n",
    "real_duration_samples = duration * SR // ae_ratio * ae_ratio\n",
    "real_duration_time = real_duration_samples / SR\n",
    "z_length = real_duration_samples // ae_ratio\n",
    "\n",
    "x1 = torch.tensor(x1)\n",
    "z1 = model.ae_encode(x1)[..., :z_length]\n",
    "\n",
    "# Get the midi\n",
    "import pretty_midi\n",
    "\n",
    "midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "ae_ratio = gin.query_parameter(\"utils.collate_fn.ae_ratio\")\n",
    "\n",
    "full_length = dataset[0][\"z\"].shape[-1]\n",
    "times = times = np.linspace(offset_midi, offset_midi + real_duration_time,\n",
    "                            z_length)\n",
    "pr = midi.get_piano_roll(times=times)\n",
    "pr = pr / 127\n",
    "# pr = pr[..., :n_signal]\n",
    "pr = torch.from_numpy(pr).float().reshape(1, 128, -1)\n",
    "\n",
    "print(\"Piano Roll\")\n",
    "plt.imshow(pr[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = 15  #Number of diffusion steps\n",
    "guidance_timbre = 2.0  #Classifier free guidance strength for timbre\n",
    "guidance_structure = 3.0  #Classifier free guidance strength strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structure representation\n",
    "z_structure = pr\n",
    "\n",
    "# Compute timbre representation -  Timbre must be computed on latent codes of length n_signal - by default we use the first n_signal elements\n",
    "\n",
    "z_timbre = model.timbre(z1[..., :n_signal])\n",
    "\n",
    "# Sample initial noise\n",
    "noise = torch.randn_like(z1)\n",
    "\n",
    "print(\"Transfer\")\n",
    "\n",
    "xS = model.sample(\n",
    "    noise=noise,\n",
    "    z_structure=z_structure,\n",
    "    z_timbre=z_timbre,\n",
    "    nb_steps=nb_steps,\n",
    "    guidance_structure=guidance_structure,\n",
    "    guidance_timbre=guidance_timbre,\n",
    ")\n",
    "\n",
    "audio_out = model.ae_decode(xS)\n",
    "display(Audio(audio_out, rate=SR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
