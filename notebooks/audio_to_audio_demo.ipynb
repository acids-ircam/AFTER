{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-to-audio generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "\n",
    "gin.enter_interactive_mode()\n",
    "from IPython.display import display, Audio\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Model can be loaded from a training checkpoint (.pt file stored by default in ./after_runs/#model_name), or from an already exported .ts file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model from a training Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "step = None  #Use None for last step\n",
    "autoencoder_path = \"\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, emb_model, device=device):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.emb_model = emb_model\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def ae_encode(self, x):\n",
    "        if len(x.shape) > 1:\n",
    "            x = x.reshape(x.shape[0], 1, -1)  # Flatten the input\n",
    "        else:\n",
    "            x = x.reshape(1, 1, -1)\n",
    "        return self.emb_model.encode(x.to(self.device))\n",
    "\n",
    "    def ae_decode(self, z):\n",
    "        return self.emb_model.decode(z.to(self.device)).cpu().squeeze()\n",
    "\n",
    "    def timbre(self, z):\n",
    "        return self.model.encoder(z.to(self.device))\n",
    "\n",
    "    def structure(self, z):\n",
    "        return self.model.encoder_time(z.to(self.device))\n",
    "\n",
    "    def sample(self, noise, z_structure, z_timbre, guidance_timbre,\n",
    "               guidance_structure, nb_steps):\n",
    "\n",
    "        zout = self.model.sample(noise,\n",
    "                                 time_cond=z_structure,\n",
    "                                 cond=z_timbre,\n",
    "                                 nb_steps=nb_steps,\n",
    "                                 guidance_structure=guidance_structure,\n",
    "                                 guidance_timbre=guidance_timbre)\n",
    "        return zout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from after.diffusion import RectifiedFlow\n",
    "\n",
    "if step is None:\n",
    "    files = os.listdir(model_path)\n",
    "    files = [f for f in files if f.startswith(\"checkpoint\")]\n",
    "    steps = [f.split(\"_\")[-2].replace(\"checkpoint\", \"\") for f in files]\n",
    "    step = max([int(s) for s in steps])\n",
    "    checkpoint_file = \"checkpoint\" + str(step) + \"_EMA.pt\"\n",
    "else:\n",
    "    checkpoint_file = \"checkpoint\" + str(step) + \"_EMA.pt\"\n",
    "\n",
    "checkpoint_file = os.path.join(model_path, checkpoint_file)\n",
    "config = os.path.join(model_path, \"config.gin\")\n",
    "\n",
    "# Parse config\n",
    "gin.parse_config_file(config)\n",
    "SR = gin.query_parameter(\"%SR\")\n",
    "n_signal = gin.query_parameter(\"%N_SIGNAL\")\n",
    "latent_size = gin.query_parameter(\"%IN_SIZE\")\n",
    "# Emb model\n",
    "\n",
    "# Instantiate model\n",
    "blender = RectifiedFlow(device=device)\n",
    "\n",
    "# Load checkpoints\n",
    "state_dict = torch.load(checkpoint_file, map_location=\"cpu\")[\"model_state\"]\n",
    "blender.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Emb model\n",
    "emb_model = torch.jit.load(autoencoder_path).eval()\n",
    "\n",
    "dummy = torch.randn(1, 1, 8192)  # Dummy input for model initialization\n",
    "z = emb_model.encode(dummy)\n",
    "\n",
    "ae_ratio = dummy.shape[-1] / z.shape[-1]\n",
    "\n",
    "# Send to device\n",
    "blender = blender.eval()\n",
    "\n",
    "model = CheckpointModel(blender, emb_model, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model from a torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_path = \"\"\n",
    "autoencoder_path = \"\"\n",
    "\n",
    "ts_model = torch.jit.load(ts_path)\n",
    "emb_model = torch.jit.load(autoencoder_path).eval(\n",
    ")  # You can set None if the autoencoder path is not known, but it will use the streaming model embedded in the .ts which has some latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ts_model, emb_model=None):\n",
    "        super().__init__()\n",
    "        self.model = ts_model\n",
    "        self.emb_model = emb_model\n",
    "\n",
    "    def ae_encode(self, x):\n",
    "        if len(x.shape) > 1:\n",
    "            x = x.reshape(x.shape[0], 1, -1)  # Flatten the input\n",
    "        else:\n",
    "            x = x.reshape(1, 1, -1)\n",
    "\n",
    "        if self.emb_model is not None:\n",
    "            return self.emb_model.encode(x)\n",
    "        return self.model.emb_model_structure.encode(x)\n",
    "\n",
    "    def ae_decode(self, z):\n",
    "        if self.emb_model is not None:\n",
    "            return self.emb_model.decode(z).squeeze().cpu()\n",
    "        return self.model.emb_model_structure.decode(z).squeeze().cpu()\n",
    "\n",
    "    def timbre(self, z):\n",
    "        return self.model.encoder.forward_stream(z)\n",
    "\n",
    "    def structure(self, z):\n",
    "        return self.model.encoder_time.forward_stream(z)\n",
    "\n",
    "    def sample(self, noise, z_structure, z_timbre, guidance_timbre,\n",
    "               guidance_structure, nb_steps):\n",
    "\n",
    "        self.model.set_guidance_timbre(guidance_timbre)\n",
    "        self.model.set_guidance_structure(guidance_structure)\n",
    "        self.model.set_nb_steps(nb_steps)\n",
    "\n",
    "        zout = self.model.sample(noise, time_cond=z_structure, cond=z_timbre)\n",
    "\n",
    "        return zout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSModel(ts_model, emb_model=emb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Samples\n",
    "\n",
    "You can either load a sample from a prepared lmdb database, or directly from audio files (below). By defaut, audio files are cut to the training length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load audio from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from after.dataset import SimpleDataset\n",
    "\n",
    "db_path = \"\"\n",
    "dataset = SimpleDataset(path=db_path, keys=[\"z\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = dataset[1][\"z\"]  #[..., :n_signal]\n",
    "z2 = dataset[2890][\"z\"]  #[..., :n_signal]\n",
    "\n",
    "z1, z2 = torch.tensor(z1).unsqueeze(0), torch.tensor(z2).unsqueeze(0)\n",
    "\n",
    "x1, x2 = model.ae_decode(z1), model.ae_decode(z2)\n",
    "\n",
    "display(Audio(x1, rate=SR))\n",
    "display(Audio(x2, rate=SR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load audio from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"\"\n",
    "path2 = \"\"\n",
    "\n",
    "x1, _ = librosa.load(path1, sr=SR, mono=True)\n",
    "\n",
    "x2, _ = librosa.load(path2, sr=SR, mono=True)\n",
    "print(x1.shape)\n",
    "display(Audio(x1, rate=SR))\n",
    "display(Audio(x2, rate=SR))\n",
    "\n",
    "x1, x2 = torch.tensor(x1), torch.tensor(x2)\n",
    "z1, z2 = model.ae_encode(x1), model.ae_encode(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = 10  #Number of diffusion steps\n",
    "guidance_timbre = 2.0  #Classifier free guidance strength for timbre\n",
    "guidance_structure = 1.0  #Classifier free guidance strength strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structure representation\n",
    "z_structure1, z_structure2 = model.structure(z1), model.structure(z2)\n",
    "\n",
    "# Compute timbre representation -  Timbre must be computed on latent codes of length n_signal - by default we use the first n_signal elements\n",
    "\n",
    "z_timbre1, z_timbre2 = model.timbre(z1[..., :n_signal]), model.timbre(\n",
    "    z2[..., :n_signal])\n",
    "\n",
    "z_structure = z_structure2\n",
    "z_timbre = z_timbre1\n",
    "\n",
    "# Sample initial noise\n",
    "noise = torch.randn_like(z1)\n",
    "\n",
    "print(\"Transfer\")\n",
    "\n",
    "xS = model.sample(\n",
    "    noise=noise,\n",
    "    z_structure=z_structure,\n",
    "    z_timbre=z_timbre,\n",
    "    nb_steps=nb_steps,\n",
    "    guidance_structure=guidance_structure,\n",
    "    guidance_timbre=guidance_timbre,\n",
    ")\n",
    "\n",
    "audio_out = model.ae_decode(xS)\n",
    "display(Audio(audio_out, rate=SR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
